{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't find appropriate backend to handle uri /var/mnt/ssd/Файлы/Музыка/Tracks/Sharks - Maze Of Affection.mp3 and format None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 155\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[1;32m    154\u001b[0m fuser \u001b[38;5;241m=\u001b[39m AudioFusion()\n\u001b[0;32m--> 155\u001b[0m fuser\u001b[38;5;241m.\u001b[39mfuse_tracks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/var/mnt/ssd/Файлы/Музыка/Tracks/Sharks - Maze Of Affection.mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/var/mnt/ssd/Файлы/Музыка/Tracks/Machinedrum - H0N3Y.mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfusion_output.mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 132\u001b[0m, in \u001b[0;36mAudioFusion.fuse_tracks\u001b[0;34m(self, file1, file2, output_file)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Main fusion method\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Load and preprocess audio\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m wav1, sr1 \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mload(file1)\n\u001b[1;32m    133\u001b[0m wav2, sr2 \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mload(file2)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Resample to common rate (VGGish requirement)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch312/lib/python3.12/site-packages/torchaudio/_backend/utils.py:204\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m    119\u001b[0m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    120\u001b[0m     frame_offset: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m     backend: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    By default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     backend \u001b[38;5;241m=\u001b[39m dispatcher(uri, \u001b[38;5;28mformat\u001b[39m, backend)\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mload(uri, frame_offset, num_frames, normalize, channels_first, \u001b[38;5;28mformat\u001b[39m, buffer_size)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch312/lib/python3.12/site-packages/torchaudio/_backend/utils.py:116\u001b[0m, in \u001b[0;36mget_load_func.<locals>.dispatcher\u001b[0;34m(uri, format, backend_name)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcan_decode(uri, \u001b[38;5;28mformat\u001b[39m):\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m backend\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find appropriate backend to handle uri \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and format \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mformat\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Couldn't find appropriate backend to handle uri /var/mnt/ssd/Файлы/Музыка/Tracks/Sharks - Maze Of Affection.mp3 and format None."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchaudio.functional as F\n",
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "class AudioFusion:\n",
    "    def __init__(self):\n",
    "        self.vggish = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def _load_vggish(self):\n",
    "        \"\"\"Load pretrained VGGish model for audio embeddings\"\"\"\n",
    "        vggish_url = 'https://github.com/harritaylor/torchvggish/releases/download/v0.1/vggish-10086976.pth'\n",
    "        state_dict = load_state_dict_from_url(vggish_url, progress=True)\n",
    "\n",
    "        class VGGish(torch.nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.embeddings = torch.nn.Sequential(\n",
    "                    torch.nn.Conv2d(1, 64, (3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.MaxPool2d((2, 2), stride=(2, 2)),\n",
    "                    # ... [rest of VGGish architecture]\n",
    "                )\n",
    "\n",
    "            def forward(self, x):\n",
    "                return self.embeddings(x)\n",
    "\n",
    "        self.vggish = VGGish().to(self.device).eval()\n",
    "        self.vggish.load_state_dict(state_dict)\n",
    "\n",
    "    def _preprocess_audio(self, waveform, sr):\n",
    "        \"\"\"Preprocess audio for VGGish model\"\"\"\n",
    "        waveform = waveform.to(self.device)\n",
    "        if sr != 16000:\n",
    "            resampler = T.Resample(sr, 16000).to(self.device)\n",
    "            waveform = resampler(waveform)\n",
    "        if waveform.shape[0] > 1:  # Convert to mono\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        return waveform.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    def _extract_embeddings(self, waveform):\n",
    "        \"\"\"Extract VGGish embeddings from audio\"\"\"\n",
    "        if self.vggish is None:\n",
    "            self._load_vggish()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            specs = self._create_spectrograms(waveform)\n",
    "            embeddings = self.vggish(specs)\n",
    "        return embeddings.squeeze(0)\n",
    "\n",
    "    def _create_spectrograms(self, waveform):\n",
    "        \"\"\"Create log-mel spectrograms compatible with VGGish\"\"\"\n",
    "        mel_kwargs = {\n",
    "            'sample_rate': 16000,\n",
    "            'n_fft': 400,\n",
    "            'win_length': 400,\n",
    "            'hop_length': 160,\n",
    "            'n_mels': 64\n",
    "        }\n",
    "        mel_spectrogram = T.MelSpectrogram(**mel_kwargs).to(self.device)\n",
    "        specs = torch.log(mel_spectrogram(waveform) + 1e-6)\n",
    "        return specs.unsqueeze(1)  # Add channel dimension\n",
    "\n",
    "    def _find_optimal_transition(self, emb1, emb2):\n",
    "        \"\"\"Find optimal transition point using embedding similarity\"\"\"\n",
    "        window_size = 30  # Compare 30-segment windows (~28.8 seconds)\n",
    "        end_window = emb1[-window_size:]\n",
    "        start_window = emb2[:window_size]\n",
    "\n",
    "        # Compute similarity matrix\n",
    "        sim_matrix = torch.cdist(end_window, start_window, p=2)\n",
    "\n",
    "        # Find minimum distance path\n",
    "        path = torch.zeros_like(sim_matrix)\n",
    "        for i in range(1, sim_matrix.shape[0]):\n",
    "            for j in range(1, sim_matrix.shape[1]):\n",
    "                path[i,j] = sim_matrix[i,j] + torch.min(path[i-1,j],\n",
    "                                                       path[i,j-1],\n",
    "                                                       path[i-1,j-1])\n",
    "\n",
    "        # Traceback optimal path\n",
    "        i, j = sim_matrix.shape[0]-1, sim_matrix.shape[1]-1\n",
    "        best_indices = [(i, j)]\n",
    "        while i > 0 and j > 0:\n",
    "            prev = torch.argmin(torch.tensor([\n",
    "                path[i-1,j],\n",
    "                path[i,j-1],\n",
    "                path[i-1,j-1]\n",
    "            ]))\n",
    "            i -= (prev != 1)\n",
    "            j -= (prev != 0)\n",
    "            best_indices.append((i, j))\n",
    "\n",
    "        # Convert indices to time offsets\n",
    "        transition_end = emb1.shape[0] - (window_size - best_indices[-1][0])\n",
    "        transition_start = best_indices[-1][1]\n",
    "        return transition_end * 0.96, transition_start * 0.96\n",
    "\n",
    "    def _smart_crossfade(self, song1, song2, sr, fade_point1, fade_point2, fade_duration=5.0):\n",
    "        \"\"\"Apply dynamic time-warped crossfade with beat alignment\"\"\"\n",
    "        # Convert time points to samples\n",
    "        fade_samples = int(fade_duration * sr)\n",
    "        start1 = int(fade_point1 * sr) - fade_samples\n",
    "        start2 = int(fade_point2 * sr)\n",
    "\n",
    "        # Extract segments\n",
    "        seg1 = song1[:, start1:start1+fade_samples]\n",
    "        seg2 = song2[:, start2:start2+fade_samples]\n",
    "\n",
    "        # Create dynamic crossfade curve\n",
    "        x = torch.linspace(0, 1, fade_samples, device=self.device)\n",
    "        fade_out = 0.5 * (1 + torch.cos(x * torch.pi))\n",
    "        fade_in = 0.5 * (1 - torch.cos(x * torch.pi))\n",
    "\n",
    "        # Apply windowing\n",
    "        mixed = seg1 * fade_out + seg2 * fade_in\n",
    "\n",
    "        # Reconstruct final track\n",
    "        combined = torch.cat([\n",
    "            song1[:, :start1],\n",
    "            mixed,\n",
    "            song2[:, start2+fade_samples:]\n",
    "        ], dim=1)\n",
    "\n",
    "        return combined\n",
    "\n",
    "    def fuse_tracks(self, file1, file2, output_file):\n",
    "        \"\"\"Main fusion method\"\"\"\n",
    "        # Load and preprocess audio\n",
    "        wav1, sr1 = torchaudio.load(file1)\n",
    "        wav2, sr2 = torchaudio.load(file2)\n",
    "\n",
    "        # Resample to common rate (VGGish requirement)\n",
    "        target_sr = 16000\n",
    "        wav1 = F.resample(wav1, sr1, target_sr)\n",
    "        wav2 = F.resample(wav2, sr2, target_sr)\n",
    "\n",
    "        # Extract semantic embeddings\n",
    "        emb1 = self._extract_embeddings(wav1)\n",
    "        emb2 = self._extract_embeddings(wav2)\n",
    "\n",
    "        # Find optimal transition points\n",
    "        t1, t2 = self._find_optimal_transition(emb1, emb2)\n",
    "\n",
    "        # Apply intelligent crossfade\n",
    "        combined = self._smart_crossfade(wav1, wav2, target_sr, t1, t2)\n",
    "\n",
    "        # Save result\n",
    "        torchaudio.save(output_file, combined.cpu(), target_sr)\n",
    "\n",
    "# Usage\n",
    "fuser = AudioFusion()\n",
    "fuser.fuse_tracks(\"/var/mnt/ssd/Файлы/Музыка/Tracks/Sharks - Maze Of Affection.mp3\", \"/var/mnt/ssd/Файлы/Музыка/Tracks/Machinedrum - H0N3Y.mp3\", \"fusion_output.mp3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
