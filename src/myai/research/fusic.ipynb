{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myai.imports import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models, transforms as tv_transforms\n",
    "from myai.loaders.audio import audioreadtensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(file_path, target_sr=22050)->torch.Tensor:\n",
    "    try:audio, sr = torchaudio.load(file_path)\n",
    "    except Exception:\n",
    "        audio, sr = audioreadtensor(file_path)\n",
    "    if sr != target_sr:\n",
    "        resampler = transforms.Resample(sr, target_sr)\n",
    "        audio = resampler(audio)\n",
    "    audio = audio.mean(dim=0, keepdim=True)  # Convert to mono\n",
    "    return audio\n",
    "\n",
    "def compute_stft(audio, n_fft=2048, hop_length=512)->tuple[torch.Tensor,torch.Tensor]:\n",
    "    window = torch.hann_window(n_fft)\n",
    "    stft = torch.stft(audio, n_fft, hop_length, window=window, return_complex=True)\n",
    "    magnitude = torch.abs(stft)\n",
    "    phase = torch.angle(stft)\n",
    "    return magnitude, phase\n",
    "\n",
    "def gram_matrix(feature):\n",
    "    batch_size, channels, height, width = feature.size()\n",
    "    features = feature.view(channels, height * width)\n",
    "    G = torch.mm(features, features.t())\n",
    "    return G.div(channels * height * width)\n",
    "\n",
    "class VGGFeatures(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.vgg = models.vgg19(pretrained=True).features\n",
    "        self.layers = layers\n",
    "        self.layer_map = {\n",
    "            '3': 'conv1_2', '8': 'conv2_2', '17': 'conv3_4',\n",
    "            '26': 'conv4_4', '35': 'conv5_4'\n",
    "        }\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for name, module in self.vgg.named_children():\n",
    "            x = module(x)\n",
    "            if str(name) in self.layers:\n",
    "                features.append(x)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_audio = load_audio(\"/var/mnt/ssd/Файлы/Музыка/Tracks/Thook - Leftintide.mp3\")\n",
    "style_audio = load_audio(\"/var/mnt/ssd/Файлы/Музыка/Tracks/Sorza - Visions of what could be.mp3\")\n",
    "\n",
    "# stfts\n",
    "content_magnitude, content_phase = compute_stft(content_audio)\n",
    "style_magnitude, style_phase = compute_stft(style_audio)\n",
    "\n",
    "# normlize\n",
    "max_magnitude:torch.Tensor = max(content_magnitude.max(), style_magnitude.max())\n",
    "content_magnitude_normalized = content_magnitude / max_magnitude\n",
    "style_magnitude_normalized = style_magnitude / max_magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/home/jj/distrobox/arch/miniconda3/envs/pytorch312/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/var/home/jj/distrobox/arch/miniconda3/envs/pytorch312/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /var/home/jj/distrobox/arch/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
      "100%|██████████| 548M/548M [07:29<00:00, 1.28MB/s] \n"
     ]
    }
   ],
   "source": [
    "# make into images\n",
    "content_spectrogram = content_magnitude_normalized.unsqueeze(0).repeat(1, 3, 1, 1)\n",
    "style_spectrogram = style_magnitude_normalized.unsqueeze(0).repeat(1, 3, 1, 1)\n",
    "\n",
    "# normalize\n",
    "# normalize = tv_transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "content_spectrogram = znormalize(content_spectrogram)\n",
    "style_spectrogram = znormalize(style_spectrogram)\n",
    "\n",
    "# vgg\n",
    "content_layer = '26'\n",
    "style_layers = ['1', '6', '11', '20', '29']\n",
    "vgg = VGGFeatures([content_layer] + style_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OKAY I DONT HAVE ENOUGH MEMORY FOR THIS BIG SAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features\n",
    "content_features = vgg(content_spectrogram)\n",
    "style_features = vgg(style_spectrogram)\n",
    "\n",
    "# separate content and style features\n",
    "content_target = content_features[0].detach()\n",
    "style_targets = [gram_matrix(feat.detach()) for feat in style_features[1:]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
